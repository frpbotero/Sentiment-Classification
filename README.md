# Classifica√ß√£o de Texto com CNN - An√°lise de Sentimento no Twitter

## üìÖ Projeto de Aprendizado de M√°quina - NLP com Deep Learning

### √î Equipe:

* Felipe Botero
* Claudio Sampaio
* Henrique Cavalcante
* Tiago Souza

---

## üìñ Tema

Classifica√ß√£o de sentimentos em mensagens do Twitter utilizando Redes Neurais Convolucionais (CNNs) com TensorFlow/Keras.

---

## ¬ß1. Importa√ß√£o das Bibliotecas Necess√°rias

Utilizamos bibliotecas para:

* Manipula√ß√£o de dados: `pandas`, `numpy`
* Visualiza√ß√£o: `matplotlib`, `seaborn`
* Modelagem: `tensorflow.keras`, `sklearn`

```python
# Author: Yousef Mohamed
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.layers import Dense, ReLU, Embedding, BatchNormalization, Concatenate, Conv1D, GlobalMaxPooling1D, Dropout
from tensorflow.keras.models import Sequential, Model
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
```

---

## ¬ß2. Leitura e Processamento dos Dados

### 2.1 Leitura dos dados

* O dataset foi dividido em treino, valida√ß√£o e teste.
* Cada linha possui uma frase e o sentimento associado.

### 2.2 Visualiza√ß√£o da distribui√ß√£o dos r√≥tulos

* Gera√ß√£o de gr√°ficos de pizza para visualizar o desbalanceamento.

### 2.3 Balanceamento dos dados

* R√≥tulos menos frequentes ("love" e "surprise") foram removidos.
* Os demais foram amostrados para garantir proporcionalidade.

### 2.4 Codifica√ß√£o e Tokeniza√ß√£o

* `LabelEncoder` transforma os r√≥tulos de texto em inteiros.
* `Tokenizer` converte texto para sequ√™ncias num√©ricas.
* `pad_sequences` garante tamanho fixo (maxlen=50).

---

## ¬ß3. Constru√ß√£o do Modelo de Deep Learning

### 3.1 Arquitetura da Rede

* Duas branches paralelas (CNNs) com:

  * `Embedding` layer
  * `Conv1D + BatchNormalization + ReLU`
  * `Dropout` + `GlobalMaxPooling1D`
* Concatenadas e passadas por camadas densas para classifica√ß√£o.

### 3.2 Compila√ß√£o do Modelo

* Otimizador: `Adamax`
* M√©tricas: `accuracy`, `precision`, `recall`
* Perda: `categorical_crossentropy`

### 3.3 Treinamento

* Epochs: 25
* Batch size: 256
* Valida√ß√£o em tempo real durante o treinamento

---

## ¬ß4. Avalia√ß√£o e Visualiza√ß√£o dos Resultados

### 4.1 Avalia√ß√£o Quantitativa

* Avalia√ß√£o nos conjuntos de treino e teste.
* Impress√£o de `accuracy`, `precision` e `recall`.

### 4.2 Visualiza√ß√£o

* Gr√°ficos de curva de aprendizado por epoch para:

  * `Loss`
  * `Accuracy`
  * `Precision`
  * `Recall`

### 4.3 Matriz de Confus√£o e Relat√≥rio

* `confusion_matrix` + `classification_report`
* R√≥tulos: `anger`, `fear`, `joy`, `sadness`

---

## ¬ß5. Salvamento do Modelo

* Tokenizer salvo com `pickle`.
* Modelo salvo no formato `.keras` (recomendado pelo Keras).

```python
model.save('nlp.keras')
with open('tokenizer.pkl', 'wb') as tokenizer_file:
    pickle.dump(tokenizer, tokenizer_file)
```

---

## ¬ß6. Fun√ß√£o de Previs√£o

* Fun√ß√£o `predict(text, model_path, token_path)`:

  * Carrega modelo e tokenizer.
  * Gera sequ√™ncia para novo texto.
  * Retorna e plota probabilidade para cada emo√ß√£o.

Exemplo:

```python
txt = 'I am very happy to finish this project'
predict(txt, 'nlp.keras', 'tokenizer.pkl')
```

---

## ‚úÖ Resultados

* Accuracy no teste: **93.8%**
* Modelo robusto com boa capacidade de generaliza√ß√£o
* CNN demonstrou excelente desempenho em classifica√ß√£o de sentimentos curtos no estilo Twitter

---

## üéì Refer√™ncia Base

Dataset: [Emotions Dataset for NLP](https://www.kaggle.com/datasets/danofer/emotion-classification)

Autor base do notebook: Yousef Mohamed (Kaggle)

Projeto adaptado e expandido para fins acad√™micos.
